{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3379176f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from sklearn.metrics import mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8798e5",
   "metadata": {},
   "source": [
    "#### Objective\n",
    "\n",
    "- We have divided the data into 30 clusters, now goal is to predict the demand in neighbouring region within 1 miles (or next 15mins).\n",
    "- Now we need to prepare the dataset in way, for a given region and time interval (of 15 mins), we have count of pickups.\n",
    "- if we just bin the dataset, there will be sudden pick and down (spiky nature) in dataset, this spiky nature can adversely impact the model predictions, thus we need to do smoothing on dataset, which can help is captuing the essence/trend of data. there are two ways to do smoothing - \n",
    "    - Moving Average (MA) - if we take a window of 3 days, it will do avg of current day and two previous day, but it will be giving equal weightage to all 3 days. more the window size, more smooth will be the average and visa versa.\n",
    "    - EWMA - It do not provide equal weightage to each data point, it gives more weightage to current weightage and less to previous, and it can be controlled via Beta. if beta is high more weightage to current observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "480788bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jan_path = \"../data/raw/yellow_tripdata_2016-01.csv\"\n",
    "df_feb_path = \"../data/raw/yellow_tripdata_2016-02.csv\"\n",
    "df_mar_path = \"../data/raw/yellow_tripdata_2016-03.csv\"\n",
    "\n",
    "# load the dataframes\n",
    "\n",
    "df_jan = dd.read_csv(df_jan_path, assume_missing=True, usecols= ['trip_distance', 'tpep_pickup_datetime', 'pickup_longitude',\n",
    "       'pickup_latitude','dropoff_longitude', 'dropoff_latitude', 'fare_amount'], parse_dates=[\"tpep_pickup_datetime\"])\n",
    "\n",
    "df_feb = dd.read_csv(df_feb_path, assume_missing=True, usecols= ['trip_distance', 'tpep_pickup_datetime', 'pickup_longitude',\n",
    "       'pickup_latitude','dropoff_longitude', 'dropoff_latitude', 'fare_amount'], parse_dates=[\"tpep_pickup_datetime\"])\n",
    "\n",
    "\n",
    "df_mar = dd.read_csv(df_mar_path, assume_missing=True, usecols= ['trip_distance', 'tpep_pickup_datetime', 'pickup_longitude',\n",
    "       'pickup_latitude','dropoff_longitude', 'dropoff_latitude', 'fare_amount'], parse_dates=[\"tpep_pickup_datetime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d8d8914c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.concat([df_jan, df_feb, df_mar], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdaa7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing outlier and wrong entires, which we detected in Outlier_Removal.ipynb\n",
    "\n",
    "min_latitude = 40.60\n",
    "max_latitude = 40.85\n",
    "min_longitude = -74.05\n",
    "max_longitude = -73.70\n",
    "\n",
    "min_fare = 0.50 \n",
    "max_fare = 125 \n",
    "\n",
    "min_distance = 0.25 \n",
    "max_distance = 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2287c39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## removing outliers\n",
    "\n",
    "df_final = (\n",
    "    df\n",
    "    .loc[\n",
    "        (\n",
    "            # removing coordinate that are not inside/on bounding box \n",
    "            df['pickup_latitude'].between(min_latitude, max_latitude, inclusive='both') & \n",
    "            df['pickup_longitude'].between(min_longitude, max_longitude, inclusive='both') &\n",
    "            df['dropoff_latitude'].between(min_latitude, max_latitude, inclusive='both') &\n",
    "            df['dropoff_longitude'].between(min_longitude, max_longitude, inclusive='both') &\n",
    "\n",
    "            # removing outliers present in fare and trip distance\n",
    "            df['fare_amount'].between(min_fare, max_fare, inclusive='both') &\n",
    "            df['trip_distance'].between(min_distance, max_distance, inclusive='both')\n",
    "        ),\n",
    "        :\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b69c25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing not useful columns\n",
    "\n",
    "df_final = df_final.drop(columns=['trip_distance', 'dropoff_longitude', 'dropoff_latitude', 'fare_amount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fdcc0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c:\\\\Users\\\\aksha\\\\OneDrive\\\\Desktop\\\\Urban Fleet Equilibrium Engine via Dynamic Geo-Clustering\\\\data\\\\interim\\\\processing_data.csv']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.to_csv('../data/interim/processing_data.csv', single_file=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eec85452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv():\n",
    "    return pd.read_csv('c:\\\\Users\\\\aksha\\\\OneDrive\\\\Desktop\\\\Urban Fleet Equilibrium Engine via Dynamic Geo-Clustering\\\\data\\\\interim\\\\processing_data.csv', chunksize=100000, usecols=['pickup_latitude', 'pickup_longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a8c025",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "for chunk in read_csv():\n",
    "    scaler.partial_fit(chunk)\n",
    "\n",
    "kmeans = MiniBatchKMeans(n_clusters=30, n_init=10, random_state=42)\n",
    "\n",
    "for chunk in read_csv():\n",
    "    scaled_chunk = scaler.transform(chunk)\n",
    "    kmeans.partial_fit(scaled_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "58f69fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_df = df_final.iloc[:, 1:].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4d080359",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_location_df = scaler.transform(location_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ef6cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating cluster for each data points\n",
    "\n",
    "cluster_predictions = kmeans.predict(scaled_location_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f5ff45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a column 'region' to assign the cluster predictions\n",
    "\n",
    "time_series_data = df_final['tpep_pickup_datetime'].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7149cf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_data = time_series_data.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "133ac2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_data['region'] = cluster_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4463bb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_data.to_csv('../data/interim/time_series.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ce6d4706",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_data.set_index('tpep_pickup_datetime', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d371f326",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_grp = time_series_data.groupby('region')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5fad78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "region  tpep_pickup_datetime\n",
       "0       2016-01-01 00:00:00     186\n",
       "        2016-01-01 00:15:00     496\n",
       "        2016-01-01 00:30:00     508\n",
       "        2016-01-01 00:45:00     470\n",
       "        2016-01-01 01:00:00     489\n",
       "                               ... \n",
       "29      2016-03-31 22:45:00      74\n",
       "        2016-03-31 23:00:00      67\n",
       "        2016-03-31 23:15:00      67\n",
       "        2016-03-31 23:30:00      60\n",
       "        2016-03-31 23:45:00      70\n",
       "Name: region, Length: 262080, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group by 15 min interval (1-1.5 miles)\n",
    "resampled_data = (\n",
    "    region_grp['region']\n",
    "    .resample('15min')\n",
    "    .count()\n",
    ")\n",
    "\n",
    "resampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82c0991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming the col name\n",
    "\n",
    "resampled_data.name = 'total_pickups'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1c2ae0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_data = resampled_data.reset_index(level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d1688478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>total_pickups</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-01 00:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 00:15:00</th>\n",
       "      <td>0</td>\n",
       "      <td>496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 00:30:00</th>\n",
       "      <td>0</td>\n",
       "      <td>508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 00:45:00</th>\n",
       "      <td>0</td>\n",
       "      <td>470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 01:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-03-31 22:45:00</th>\n",
       "      <td>29</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-03-31 23:00:00</th>\n",
       "      <td>29</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-03-31 23:15:00</th>\n",
       "      <td>29</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-03-31 23:30:00</th>\n",
       "      <td>29</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-03-31 23:45:00</th>\n",
       "      <td>29</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>262080 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      region  total_pickups\n",
       "tpep_pickup_datetime                       \n",
       "2016-01-01 00:00:00        0            186\n",
       "2016-01-01 00:15:00        0            496\n",
       "2016-01-01 00:30:00        0            508\n",
       "2016-01-01 00:45:00        0            470\n",
       "2016-01-01 01:00:00        0            489\n",
       "...                      ...            ...\n",
       "2016-03-31 22:45:00       29             74\n",
       "2016-03-31 23:00:00       29             67\n",
       "2016-03-31 23:15:00       29             67\n",
       "2016-03-31 23:30:00       29             60\n",
       "2016-03-31 23:45:00       29             70\n",
       "\n",
       "[262080 rows x 2 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c340fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(4489)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there are certain combination of time interval and region, where no pickups happened, thus we are removing zero's with value of 10 pickups\n",
    "# this is important as we want continuity in our dataset\n",
    "(resampled_data['total_pickups'] == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2e354ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_val = 10\n",
    "\n",
    "resampled_data.replace({'total_pickups': {0 : epsilon_val}}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f6d0f0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = list(range(3,11,1))\n",
    "\n",
    "def calculate_best_window(windows):\n",
    "    for window in windows:\n",
    "        ind = window - 1\n",
    "        ypred = resampled_data['total_pickups'].rolling(window=window).mean().values[ind:]\n",
    "        y = resampled_data['total_pickups'].values[ind:]\n",
    "        error = mean_absolute_percentage_error(y, ypred)\n",
    "        print(f'for window value {window}, the MAPE is {round(error, 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a3f4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for window value 3, the MAPE is 0.22\n",
      "for window value 4, the MAPE is 0.26\n",
      "for window value 5, the MAPE is 0.3\n",
      "for window value 6, the MAPE is 0.34\n",
      "for window value 7, the MAPE is 0.38\n",
      "for window value 8, the MAPE is 0.42\n",
      "for window value 9, the MAPE is 0.46\n",
      "for window value 10, the MAPE is 0.5\n"
     ]
    }
   ],
   "source": [
    "# lower window value leads to reduction in loss\n",
    "calculate_best_window(window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6474a5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothing_values = np.arange(0.2,1,0.1)\n",
    "\n",
    "def calculate_best_smoothing_value(values):\n",
    "    y = resampled_data['total_pickups'].values\n",
    "    for value in values:\n",
    "        ypred = resampled_data['total_pickups'].ewm(alpha=value).mean()\n",
    "        error = mean_absolute_percentage_error(y, ypred)\n",
    "        print(f'for smoothing value {round(value, 2)}, the MAPE is {round(error, 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eae014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for smoothing value 0.2, the MAPE is 0.44\n",
      "for smoothing value 0.3, the MAPE is 0.3\n",
      "for smoothing value 0.4, the MAPE is 0.22\n",
      "for smoothing value 0.5, the MAPE is 0.17\n",
      "for smoothing value 0.6, the MAPE is 0.13\n",
      "for smoothing value 0.7, the MAPE is 0.09\n",
      "for smoothing value 0.8, the MAPE is 0.06\n",
      "for smoothing value 0.9, the MAPE is 0.03\n"
     ]
    }
   ],
   "source": [
    "# lower beta values, is more suitable\n",
    "\n",
    "calculate_best_smoothing_value(smoothing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "160eec9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>total_pickups</th>\n",
       "      <th>avg_pickups</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-01 00:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>186</td>\n",
       "      <td>186.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 00:15:00</th>\n",
       "      <td>0</td>\n",
       "      <td>496</td>\n",
       "      <td>380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 00:30:00</th>\n",
       "      <td>0</td>\n",
       "      <td>508</td>\n",
       "      <td>445.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 00:45:00</th>\n",
       "      <td>0</td>\n",
       "      <td>470</td>\n",
       "      <td>457.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 01:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>489</td>\n",
       "      <td>471.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-03-31 22:45:00</th>\n",
       "      <td>29</td>\n",
       "      <td>74</td>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-03-31 23:00:00</th>\n",
       "      <td>29</td>\n",
       "      <td>67</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-03-31 23:15:00</th>\n",
       "      <td>29</td>\n",
       "      <td>67</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-03-31 23:30:00</th>\n",
       "      <td>29</td>\n",
       "      <td>60</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-03-31 23:45:00</th>\n",
       "      <td>29</td>\n",
       "      <td>70</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>262080 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      region  total_pickups  avg_pickups\n",
       "tpep_pickup_datetime                                    \n",
       "2016-01-01 00:00:00        0            186        186.0\n",
       "2016-01-01 00:15:00        0            496        380.0\n",
       "2016-01-01 00:30:00        0            508        445.0\n",
       "2016-01-01 00:45:00        0            470        457.0\n",
       "2016-01-01 01:00:00        0            489        471.0\n",
       "...                      ...            ...          ...\n",
       "2016-03-31 22:45:00       29             74         71.0\n",
       "2016-03-31 23:00:00       29             67         69.0\n",
       "2016-03-31 23:15:00       29             67         68.0\n",
       "2016-03-31 23:30:00       29             60         65.0\n",
       "2016-03-31 23:45:00       29             70         67.0\n",
       "\n",
       "[262080 rows x 3 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resampled_data[\"avg_pickups\"] = resampled_data['total_pickups'].ewm(alpha=0.4).mean().round()\n",
    "\n",
    "resampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba84cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the dataset\n",
    "\n",
    "resampled_data.to_csv('../data/interim/final_data.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1016dd3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "penv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
